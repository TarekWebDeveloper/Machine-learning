# -*- coding: utf-8 -*-
"""ARABERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B0aC6a1mi6a7XUf0xkWsg2ZIyIou7XK0
"""

!pip install transformers==4.49.0
!pip install arabert

import torch
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from arabert import ArabertPreprocessor
from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel


# Specify the path to data file
data = '/content/dataset10.tsv'

# Read the data file into a Pandas DataFrame

Comments= pd.read_csv(data, sep='\t')

Comments

Comments.info()

Comments['Letter count']=Comments['text'].apply(len)
Comments

# Ensure the 'text' and 'target' columns are treated as strings
data = list(zip(Comments['text'].astype(str), Comments['target'].astype(str)))


# Split the dataset into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)


# Define a custom dataset class
class SentimentDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text, target = self.data[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'target': torch.tensor(int(target))  # Assuming target are integers representing the class index
        }

# Load the BERT tokenizer and model
#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
#model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)


from transformers import AutoTokenizer, AutoModel

model_name = "aubmindlab/bert-base-arabertv2"

# تحميل التوكنايزر
tokenizer = AutoTokenizer.from_pretrained(model_name)

# تحميل الموديل
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Create datasets and dataloaders
train_dataset = SentimentDataset(train_data, tokenizer)
test_dataset = SentimentDataset(test_data, tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)



# Use GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Training loop
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
num_epochs = 13

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    all_preds = []
    all_targets = []

    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):
        inputs = {key: val.to(device) for key, val in batch.items() if key != 'target'}
        targets = batch['target'].to(device)

        optimizer.zero_grad()
        outputs = model(**inputs, labels=targets)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Get model predictions
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()

        all_preds.extend(preds)
        all_targets.extend(targets.cpu().numpy())

    # Calculate accuracy
    accuracy = accuracy_score(all_targets, all_preds)

    # Print loss and accuracy
    avg_loss = total_loss / len(train_dataloader)
    print(f'Epoch {epoch + 1}/{num_epochs} - Average Loss: {avg_loss:.4f} - Accuracy: {accuracy:.4f}')

# Evaluation
model.eval()
predictions, true_targets = [], []
for batch in tqdm(test_dataloader, desc='Evaluating'):
    inputs = {key: val.to(device) for key, val in batch.items() if key != 'target'}
    targets = batch['target'].to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())
    true_targets.extend(targets.cpu().numpy())

# Calculate accuracy
accuracy = accuracy_score(true_targets, predictions)
print(f'Accuracy: {accuracy:.4f}')

# Save model locally in Colab
model.save_pretrained('/content/sentimental_analysis_model')
tokenizer.save_pretrained('/content/tokenizer')

# Calculate confusion matrix and classification report
conf_matrix = confusion_matrix(true_targets, predictions)
class_report = classification_report(true_targets, predictions)

# Display confusion matrix and classification report
print(" AraBertv2")
print(" ----------")

print("Confusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)

import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Load the BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
#cleaned="السلام عليكم كيفكن شباب"# Your input text
#cleaned="بخيرالحمد لله"
#cleaned="وعليكم السلام أهلين محمد"


# Tokenize the input text
inputs = tokenizer(cleaned, return_tensors="pt", padding=True, truncation=True, max_length=50)

# Put the model in evaluation mode (so it doesn't update weights)
model.eval()

# Pass the inputs through the model to get the output (logits)
with torch.no_grad():  # Disable gradient computation for inference
    outputs = model(**inputs)

# Get the predicted logits (raw model outputs)
logits = outputs.logits

# Apply a sigmoid to convert the logits to probabilities (if binary classification)
probs = torch.sigmoid(logits)

# Get the first probability (assuming binary classification)
x = probs[0][0].item()

print(x)

# Classify based on the threshold
if x >0.50:
    print("سلبي")
else:
    print("ايجابي")

# Plot the confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from google.colab import drive

drive.mount('/content/drive')

from transformers import AutoTokenizer, AutoModel

model_name = "aubmindlab/bert-base-arabertv2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

model.save_pretrained("/content/arabert_model")
tokenizer.save_pretrained("/content/arabert_model")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/sentimental_analysis_model

!zip -r models.zip /content/arabert_part

!split -b 100M /content/arabert_model.zip /content/drive/MyDrive/arabert_part

from google.colab import files
files.download('models.zip')